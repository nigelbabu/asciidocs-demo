<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta content="IE=edge" http-equiv="X-UA-Compatible">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <title>Gluster User Documentation Latest | Gluster Docs Project | Troubleshooting</title>

  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css">

    <link href="../../latest/_stylesheets/asciibinder.css" rel="stylesheet" />

   <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
   <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
   <!--[if lt IE 9]>
     <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
     <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
   <![endif]-->

  <link href="../../latest/_images/favicon32x32.png" rel="shortcut icon" type="text/css">
  <!--[if IE]><link rel="shortcut icon" href="../../latest/_images/favicon.ico"><![endif]-->
  <meta content="AsciiBinder" name="application-name">
</head>
<body>
  <div class="navbar navbar-default" role="navigation">
    <div class="container-fluid">
      <div class="navbar-header">
        <a class="navbar-brand" href="http://www.asciibinder.org/"><img alt="AsciiBinder" src="../../latest/_images/asciibinder-logo-horizontal.png"></a>
      </div>
    </div>
  </div>
  <div class="container">
    <p class="toggle-nav visible-xs pull-left">
      <button class="btn btn-default btn-sm" type="button" data-toggle="offcanvas">Toggle nav</button>
    </p>
    <ol class="breadcrumb">
      <li class="sitename">
        <a href="../../index.html">Home</a>
      </li>
      <li class="hidden-xs active">
        <a href="../admin-guide/index.html">Gluster User Documentation Latest</a>
      </li>
      <li class="hidden-xs active">
        <a href="../admin-guide/index.html">Gluster Docs Project</a>
      </li>
      
      <li class="hidden-xs active">
        Troubleshooting
      </li>
    </ol>
    <div class="row row-offcanvas row-offcanvas-left">
      <div class="col-xs-8 col-sm-3 col-md-3 sidebar sidebar-offcanvas">
        <ul class="nav nav-sidebar">
    <li class="nav-header">
      <a class="" href="#" data-toggle="collapse" data-target="#topicGroup0">
        <span id="tgSpan0" class="fa fa-angle-down"></span>Gluster Docs Project
      </a>
      <ul id="topicGroup0" class="collapse in list-unstyled">
            <li><a class="" href="../admin-guide/index.html">Index</a></li>
            <li><a class="" href="../admin-guide/access-control-lists.html">Access Control Lists</a></li>
            <li><a class="" href="../admin-guide/accessing-gluster-from-windows.html">Accessing Gluster from Windows</a></li>
            <li><a class="" href="../admin-guide/arbiter-volumes-and-quorum.html">Arbiter-volumes and Quorum</a></li>
            <li><a class="" href="../admin-guide/bareos.html">BareOS</a></li>
            <li><a class="" href="../admin-guide/brick-naming-conventions.html">Brick naming conventions</a></li>
            <li><a class="" href="../admin-guide/building-qemu-with-gfapi-for-debian-based-systems.html">Building QEMU with gfapi for Debian-based systems</a></li>
            <li><a class="" href="../admin-guide/cinder.html">Cinder</a></li>
            <li><a class="" href="../admin-guide/Console.html">Console</a></li>
            <li><a class="" href="../admin-guide/compiling-rpms.html">Compiling RPMs</a></li>
            <li><a class="" href="../admin-guide/coreutils.html">Coreutils</a></li>
            <li><a class="" href="../admin-guide/did-you-know.html">Did you know</a></li>
            <li><a class="" href="../admin-guide/directory-quota.html">Diectory Quota</a></li>
            <li><a class="" href="../admin-guide/distributed-geo-replication.html">Distributed Geo Replication</a></li>
            <li><a class="" href="../admin-guide/export-and-netgroup-authentication.html">Export and netgroup authentication</a></li>
            <li><a class="" href="../admin-guide/filter.html">Filter</a></li>
            <li><a class="" href="../admin-guide/geo-replication.html">Geo Replication</a></li>
            <li><a class="" href="../admin-guide/glossary.html">Glossary</a></li>
            <li><a class="" href="../admin-guide/gluster-on-zfs.html">Gluster on ZFS</a></li>
            <li><a class="" href="../admin-guide/Hadoop.html">Hadoop</a></li>
            <li><a class="" href="../admin-guide/Handling-of-users-with-many-groups.html">Handling of users with many groups</a></li>
            <li><a class="" href="../admin-guide/introduction.html">Introduction</a></li>
            <li><a class="" href="../admin-guide/iscsi.html">iSCSI</a></li>
            <li><a class="" href="../admin-guide/keystore-quickstart.html">Keystore Quickstart</a></li>
            <li><a class="" href="../admin-guide/linux-kernel-tuning.html">Linux Kernel Tuning</a></li>
            <li><a class="" href="../admin-guide/logging.html">Logging</a></li>
            <li><a class="" href="../admin-guide/managing-snapshots.html">Managing snapshots</a></li>
            <li><a class="" href="../admin-guide/managing-volumes.html">Managing Volumes</a></li>
            <li><a class="" href="../admin-guide/mandatory-locks.html">Mandatory Locks</a></li>
            <li><a class="" href="../admin-guide/monitoring-workload.html">Monitoring Workload</a></li>
            <li><a class="" href="../admin-guide/network-configuration-techniques.html">Network Configuration Techniques</a></li>
            <li><a class="" href="../admin-guide/nfs-ganesha-integration.html">NFS-Ganesha Integration</a></li>
            <li><a class="" href="../admin-guide/object-storage.html">Object storage</a></li>
            <li><a class="" href="../admin-guide/performance-testing.html">Performance Testing</a></li>
            <li><a class="" href="../admin-guide/Puppet.html">Puppet</a></li>
            <li><a class="" href="../admin-guide/rdma-transport.html">RDMA Transport</a></li>
            <li><a class="" href="../admin-guide/resolving-peer-rejected.html">Resolving Peer Rejected</a></li>
            <li><a class="" href="../admin-guide/setting-up-clients.html">Setting up Clients</a></li>
            <li><a class="" href="../admin-guide/setting-up-volumes.html">Setting up volumes</a></li>
            <li><a class="" href="../admin-guide/ssl.html">SSL</a></li>
            <li><a class="" href="../admin-guide/start-stop-daemon.html">Start Stop Daemon</a></li>
            <li><a class="" href="../admin-guide/storage-pools.html">Storage Pools</a></li>
            <li><a class="" href="../admin-guide/trash.html">Trash</a></li>
            <li><a class=" active" href="../admin-guide/troubleshooting.html">Troubleshooting</a></li>
      </ul>
    </li>
</ul>
      </div>
      <div class="col-xs-12 col-sm-9 col-md-9 main">
        <div class="page-header">
          <h2>Troubleshooting GlusterFS</h2>
        </div>
        <div class="sect1">
<h2 id="troubleshooting-glusterfs"><a class="anchor" href="#troubleshooting-glusterfs"></a>Troubleshooting GlusterFS</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This section describes how to manage GlusterFS logs and most common
troubleshooting scenarios related to GlusterFS.</p>
</div>
<div class="sect2">
<h3 id="contents"><a class="anchor" href="#contents"></a>Contents</h3>
<div class="ulist">
<ul>
<li>
<p><a href="#logs">Managing GlusterFS Logs</a></p>
</li>
<li>
<p><a href="#georep">Troubleshooting Geo-replication</a></p>
</li>
<li>
<p><a href="#posix-acls">Troubleshooting POSIX ACLs</a></p>
</li>
<li>
<p><a href="#hadoop">Troubleshooting Hadoop Compatible Storage</a></p>
</li>
<li>
<p><a href="#nfs">Troubleshooting NFS</a></p>
</li>
<li>
<p><a href="#file-locks">Troubleshooting File Locks</a></p>
<div class="literalblock">
<div class="content">
<pre>##Managing GlusterFS Logs</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="rotating-logs"><a class="anchor" href="#rotating-logs"></a>Rotating Logs</h4>
<div class="paragraph">
<p>Administrators can rotate the log file in a volume, as needed.</p>
</div>
<div class="paragraph">
<p><strong>To rotate a log file</strong></p>
</div>
<div class="literalblock">
<div class="content">
<pre>`# gluster volume log rotate `</pre>
</div>
</div>
<div class="paragraph">
<p>For example, to rotate the log file on test-volume:</p>
</div>
<div class="literalblock">
<div class="content">
<pre># gluster volume log rotate test-volume
log rotate successful</pre>
</div>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p><strong>Note</strong> When a log file is rotated, the contents of the current log file
are moved to log-file- name.epoch-time-stamp.</p>
</div>
</blockquote>
</div>
<div class="literalblock">
<div class="content">
<pre>##Troubleshooting Geo-replication</pre>
</div>
</div>
<div class="paragraph">
<p>This section describes the most common troubleshooting scenarios related
to GlusterFS Geo-replication.</p>
</div>
</div>
<div class="sect3">
<h4 id="locating-log-files"><a class="anchor" href="#locating-log-files"></a>Locating Log Files</h4>
<div class="paragraph">
<p>For every Geo-replication session, the following three log files are
associated to it (four, if the slave is a gluster volume):</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Master-log-file</strong> - log file for the process which monitors the Master
volume</p>
</li>
<li>
<p><strong>Slave-log-file</strong> - log file for process which initiates the changes in
slave</p>
</li>
<li>
<p><strong>Master-gluster-log-file</strong> - log file for the maintenance mount point
that Geo-replication module uses to monitor the master volume</p>
</li>
<li>
<p><strong>Slave-gluster-log-file</strong> - is the slave&#8217;s counterpart of it</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Master Log File</strong></p>
</div>
<div class="paragraph">
<p>To get the Master-log-file for geo-replication, use the following
command:</p>
</div>
<div class="paragraph">
<p><code>gluster volume geo-replication  config log-file</code></p>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="paragraph">
<p><code># gluster volume geo-replication Volume1 example.com:/data/remote_dir config log-file</code></p>
</div>
<div class="paragraph">
<p><strong>Slave Log File</strong></p>
</div>
<div class="paragraph">
<p>To get the log file for Geo-replication on slave (glusterd must be
running on slave machine), use the following commands:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>On master, run the following command:</p>
<div class="paragraph">
<p><code># gluster volume geo-replication Volume1 example.com:/data/remote_dir config session-owner 5f6e5200-756f-11e0-a1f0-0800200c9a66</code></p>
</div>
<div class="paragraph">
<p>Displays the session owner details.</p>
</div>
</li>
<li>
<p>On slave, run the following command:</p>
<div class="paragraph">
<p><code># gluster volume geo-replication /data/remote_dir config log-file /var/log/gluster/${session-owner}:remote-mirror.log</code></p>
</div>
</li>
<li>
<p>Replace the session owner details (output of Step 1) to the output
of the Step 2 to get the location of the log file.</p>
<div class="paragraph">
<p><code>/var/log/gluster/5f6e5200-756f-11e0-a1f0-0800200c9a66:remote-mirror.log</code></p>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="rotating-geo-replication-logs"><a class="anchor" href="#rotating-geo-replication-logs"></a>Rotating Geo-replication Logs</h4>
<div class="paragraph">
<p>Administrators can rotate the log file of a particular master-slave
session, as needed. When you run geo-replication&#8217;s <code>log-rotate</code> command,
the log file is backed up with the current timestamp suffixed to the
file name and signal is sent to gsyncd to start logging to a new log
file.</p>
</div>
<div class="paragraph">
<p><strong>To rotate a geo-replication log file</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Rotate log file for a particular master-slave session using the
following command:</p>
<div class="paragraph">
<p><code># gluster volume geo-replication  log-rotate</code></p>
</div>
<div class="paragraph">
<p>For example, to rotate the log file of master <code>Volume1</code> and slave
<code>example.com:/data/remote_dir</code> :</p>
</div>
<div class="literalblock">
<div class="content">
<pre># gluster volume geo-replication Volume1 example.com:/data/remote_dir log rotate
log rotate successful</pre>
</div>
</div>
</li>
<li>
<p>Rotate log file for all sessions for a master volume using the
following command:</p>
<div class="paragraph">
<p><code># gluster volume geo-replication  log-rotate</code></p>
</div>
<div class="paragraph">
<p>For example, to rotate the log file of master <code>Volume1</code>:</p>
</div>
<div class="literalblock">
<div class="content">
<pre># gluster volume geo-replication Volume1 log rotate
log rotate successful</pre>
</div>
</div>
</li>
<li>
<p>Rotate log file for all sessions using the following command:</p>
<div class="paragraph">
<p><code># gluster volume geo-replication log-rotate</code></p>
</div>
<div class="paragraph">
<p>For example, to rotate the log file for all sessions:</p>
</div>
<div class="literalblock">
<div class="content">
<pre># gluster volume geo-replication log rotate
log rotate successful</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="synchronization-is-not-complete"><a class="anchor" href="#synchronization-is-not-complete"></a>Synchronization is not complete</h4>
<div class="paragraph">
<p><strong>Description</strong>: GlusterFS Geo-replication did not synchronize the data
completely but still the geo- replication status displayed is OK.</p>
</div>
<div class="paragraph">
<p><strong>Solution</strong>: You can enforce a full sync of the data by erasing the
index and restarting GlusterFS Geo- replication. After restarting,
GlusterFS Geo-replication begins synchronizing all the data. All files
are compared using checksum, which can be a lengthy and high resource
utilization operation on large data sets.</p>
</div>
</div>
<div class="sect3">
<h4 id="issues-in-data-synchronization"><a class="anchor" href="#issues-in-data-synchronization"></a>Issues in Data Synchronization</h4>
<div class="paragraph">
<p><strong>Description</strong>: Geo-replication display status as OK, but the files do
not get synced, only directories and symlink gets synced with the
following error message in the log:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>[2011-05-02 13:42:13.467644] E [master:288:regjob] GMaster: failed to
sync ./some\_file\`</pre>
</div>
</div>
<div class="paragraph">
<p><strong>Solution</strong>: Geo-replication invokes rsync v3.0.0 or higher on the host
and the remote machine. You must verify if you have installed the
required version.</p>
</div>
</div>
<div class="sect3">
<h4 id="geo-replication-status-displays-faulty-very-often"><a class="anchor" href="#geo-replication-status-displays-faulty-very-often"></a>Geo-replication status displays Faulty very often</h4>
<div class="paragraph">
<p><strong>Description</strong>: Geo-replication displays status as faulty very often
with a backtrace similar to the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>2011-04-28 14:06:18.378859] E [syncdutils:131:log\_raise\_exception]
\&lt;top\&gt;: FAIL: Traceback (most recent call last): File
"/usr/local/libexec/glusterfs/python/syncdaemon/syncdutils.py", line
152, in twraptf(\*aa) File
"/usr/local/libexec/glusterfs/python/syncdaemon/repce.py", line 118, in
listen rid, exc, res = recv(self.inf) File
"/usr/local/libexec/glusterfs/python/syncdaemon/repce.py", line 42, in
recv return pickle.load(inf) EOFError</pre>
</div>
</div>
<div class="paragraph">
<p><strong>Solution</strong>: This error indicates that the RPC communication between
the master gsyncd module and slave gsyncd module is broken and this can
happen for various reasons. Check if it satisfies all the following
pre-requisites:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Password-less SSH is set up properly between the host and the remote
machine.</p>
</li>
<li>
<p>If FUSE is installed in the machine, because geo-replication module
mounts the GlusterFS volume using FUSE to sync data.</p>
</li>
<li>
<p>If the <strong>Slave</strong> is a volume, check if that volume is started.</p>
</li>
<li>
<p>If the Slave is a plain directory, verify if the directory has been
created already with the required permissions.</p>
</li>
<li>
<p>If GlusterFS 3.2 or higher is not installed in the default location
(in Master) and has been prefixed to be installed in a custom location,
configure the <code>gluster-command</code> for it to point to the exact location.</p>
</li>
<li>
<p>If GlusterFS 3.2 or higher is not installed in the default location
(in slave) and has been prefixed to be installed in a custom location,
configure the <code>remote-gsyncd-command</code> for it to point to the exact place
where gsyncd is located.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="intermediate-master-goes-to-faulty-state"><a class="anchor" href="#intermediate-master-goes-to-faulty-state"></a>Intermediate Master goes to Faulty State</h4>
<div class="paragraph">
<p><strong>Description</strong>: In a cascading set-up, the intermediate master goes to
faulty state with the following log:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>raise RuntimeError ("aborting on uuid change from %s to %s" % \\
RuntimeError: aborting on uuid change from af07e07c-427f-4586-ab9f-
4bf7d299be81 to de6b5040-8f4e-4575-8831-c4f55bd41154</pre>
</div>
</div>
<div class="paragraph">
<p><strong>Solution</strong>: In a cascading set-up the Intermediate master is loyal to
the original primary master. The above log means that the
geo-replication module has detected change in primary master. If this is
the desired behavior, delete the config option volume-id in the session
initiated from the intermediate master.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>##Troubleshooting POSIX ACLs</pre>
</div>
</div>
<div class="paragraph">
<p>This section describes the most common troubleshooting issues related to
POSIX ACLs.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>setfacl command fails with “setfacl: \&lt;file or directory name\&gt;: Operation not supported” error</pre>
</div>
</div>
<div class="paragraph">
<p>You may face this error when the backend file systems in one of the
servers is not mounted with the "-o acl" option. The same can be
confirmed by viewing the following error message in the log file of the
server "Posix access control list is not supported".</p>
</div>
<div class="paragraph">
<p><strong>Solution</strong>: Remount the backend file system with "-o acl" option.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>##Troubleshooting Hadoop Compatible Storage</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="time-sync"><a class="anchor" href="#time-sync"></a>Time Sync</h4>
<div class="paragraph">
<p><strong>Problem</strong>: Running MapReduce job may throw exceptions if the time is
out-of-sync on the hosts in the cluster.</p>
</div>
<div class="paragraph">
<p><strong>Solution</strong>: Sync the time on all hosts using ntpd program.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>##Troubleshooting NFS</pre>
</div>
</div>
<div class="paragraph">
<p>This section describes the most common troubleshooting issues related to
NFS .</p>
</div>
<div id="mount-command-on-nfs-client-fails-with-rpc-error-program-not-registered" class="paragraph">
<p>mount command on NFS client fails with “RPC Error: Program not
registered”
<sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup>^^</p>
</div>
<div class="literalblock">
<div class="content">
<pre>Start portmap or rpcbind service on the NFS server.</pre>
</div>
</div>
<div class="paragraph">
<p>This error is encountered when the server has not started correctly. On
most Linux distributions this is fixed by starting portmap:</p>
</div>
<div class="paragraph">
<p><code>$ /etc/init.d/portmap start</code></p>
</div>
<div class="paragraph">
<p>On some distributions where portmap has been replaced by rpcbind, the
following command is required:</p>
</div>
<div class="paragraph">
<p><code>$ /etc/init.d/rpcbind start</code></p>
</div>
<div class="paragraph">
<p>After starting portmap or rpcbind, gluster NFS server needs to be
restarted.</p>
</div>
<div id="nfs-server-start-up-fails-with-port-is-already-in-use-error-in-the-log-file." class="paragraph">
<p>NFS server start-up fails with “Port is already in use” error in the log
file.
<sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup></p>
</div>
<div class="paragraph">
<p>Another Gluster NFS server is running on the same machine.</p>
</div>
<div class="paragraph">
<p>This error can arise in case there is already a Gluster NFS server
running on the same machine. This situation can be confirmed from the
log file, if the following error lines exist:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>[2010-05-26 23:40:49] E [rpc-socket.c:126:rpcsvc_socket_listen] rpc-socket: binding socket failed:Address already in use
[2010-05-26 23:40:49] E [rpc-socket.c:129:rpcsvc_socket_listen] rpc-socket: Port is already in use
[2010-05-26 23:40:49] E [rpcsvc.c:2636:rpcsvc_stage_program_register] rpc-service: could not create listening connection
[2010-05-26 23:40:49] E [rpcsvc.c:2675:rpcsvc_program_register] rpc-service: stage registration of program failed
[2010-05-26 23:40:49] E [rpcsvc.c:2695:rpcsvc_program_register] rpc-service: Program registration failed: MOUNT3, Num: 100005, Ver: 3, Port: 38465
[2010-05-26 23:40:49] E [nfs.c:125:nfs_init_versions] nfs: Program init failed
[2010-05-26 23:40:49] C [nfs.c:531:notify] nfs: Failed to initialize protocols</pre>
</div>
</div>
<div class="paragraph">
<p>To resolve this error one of the Gluster NFS servers will have to be
shutdown. At this time, Gluster NFS server does not support running
multiple NFS servers on the same machine.</p>
</div>
</div>
<div class="sect3">
<h4 id="mount-command-fails-with-rpc.statd-related-error-message"><a class="anchor" href="#mount-command-fails-with-rpc.statd-related-error-message"></a>mount command fails with “rpc.statd” related error message</h4>
<div class="paragraph">
<p>If the mount command fails with the following error message:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>mount.nfs: rpc.statd is not running but is required for remote locking.
mount.nfs: Either use '-o nolock' to keep locks local, or start statd.</pre>
</div>
</div>
<div class="paragraph">
<p>For NFS clients to mount the NFS server, rpc.statd service must be
running on the clients. Start rpc.statd service by running the following
command:</p>
</div>
<div class="paragraph">
<p><code>$ rpc.statd</code></p>
</div>
</div>
<div class="sect3">
<h4 id="mount-command-takes-too-long-to-finish."><a class="anchor" href="#mount-command-takes-too-long-to-finish."></a>mount command takes too long to finish.</h4>
<div class="paragraph">
<p><strong>Start rpcbind service on the NFS client</strong></p>
</div>
<div class="paragraph">
<p>The problem is that the rpcbind or portmap service is not running on the
NFS client. The resolution for this is to start either of these services
by running the following command:</p>
</div>
<div class="paragraph">
<p><code>$ /etc/init.d/portmap start</code></p>
</div>
<div class="paragraph">
<p>On some distributions where portmap has been replaced by rpcbind, the
following command is required:</p>
</div>
<div class="paragraph">
<p><code>$ /etc/init.d/rpcbind start</code></p>
</div>
<div id="nfs-server-glusterfsd-starts-but-initialization-fails-with-nfsrpc--service-portmap-registration-of-program-failed-error-message-in-the-log." class="paragraph">
<p>NFS server glusterfsd starts but initialization fails with “nfsrpc-
service: portmap registration of program failed” error message in the
log.
<sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup>^</p>
</div>
<div class="paragraph">
<p>NFS start-up can succeed but the initialization of the NFS service can
still fail preventing clients from accessing the mount points. Such a
situation can be confirmed from the following error messages in the log
file:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>[2010-05-26 23:33:47] E [rpcsvc.c:2598:rpcsvc_program_register_portmap] rpc-service: Could notregister with portmap
[2010-05-26 23:33:47] E [rpcsvc.c:2682:rpcsvc_program_register] rpc-service: portmap registration of program failed
[2010-05-26 23:33:47] E [rpcsvc.c:2695:rpcsvc_program_register] rpc-service: Program registration failed: MOUNT3, Num: 100005, Ver: 3, Port: 38465
[2010-05-26 23:33:47] E [nfs.c:125:nfs_init_versions] nfs: Program init failed
[2010-05-26 23:33:47] C [nfs.c:531:notify] nfs: Failed to initialize protocols
[2010-05-26 23:33:49] E [rpcsvc.c:2614:rpcsvc_program_unregister_portmap] rpc-service: Could not unregister with portmap
[2010-05-26 23:33:49] E [rpcsvc.c:2731:rpcsvc_program_unregister] rpc-service: portmap unregistration of program failed
[2010-05-26 23:33:49] E [rpcsvc.c:2744:rpcsvc_program_unregister] rpc-service: Program unregistration failed: MOUNT3, Num: 100005, Ver: 3, Port: 38465</pre>
</div>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Start portmap or rpcbind service on the NFS server</strong></p>
<div class="paragraph">
<p>On most Linux distributions, portmap can be started using the following
command:</p>
</div>
<div class="paragraph">
<p><code>$ /etc/init.d/portmap start</code></p>
</div>
<div class="paragraph">
<p>On some distributions where portmap has been replaced by rpcbind, run
the following command:</p>
</div>
<div class="paragraph">
<p><code>$ /etc/init.d/rpcbind start</code></p>
</div>
<div class="paragraph">
<p>After starting portmap or rpcbind, gluster NFS server needs to be
restarted.</p>
</div>
</li>
<li>
<p><strong>Stop another NFS server running on the same machine</strong></p>
<div class="paragraph">
<p>Such an error is also seen when there is another NFS server running on
the same machine but it is not the Gluster NFS server. On Linux systems,
this could be the kernel NFS server. Resolution involves stopping the
other NFS server or not running the Gluster NFS server on the machine.
Before stopping the kernel NFS server, ensure that no critical service
depends on access to that NFS server&#8217;s exports.</p>
</div>
<div class="paragraph">
<p>On Linux, kernel NFS servers can be stopped by using either of the
following commands depending on the distribution in use:</p>
</div>
<div class="paragraph">
<p><code>$ /etc/init.d/nfs-kernel-server stop</code></p>
</div>
<div class="paragraph">
<p><code>$ /etc/init.d/nfs stop</code></p>
</div>
</li>
<li>
<p><strong>Restart Gluster NFS server</strong></p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="mount-command-fails-with-nfs-server-failed-error."><a class="anchor" href="#mount-command-fails-with-nfs-server-failed-error."></a>mount command fails with NFS server failed error.</h4>
<div class="paragraph">
<p>mount command fails with following error</p>
</div>
<div class="literalblock">
<div class="content">
<pre>*mount: mount to NFS server '10.1.10.11' failed: timed out (retrying).*</pre>
</div>
</div>
<div class="paragraph">
<p>Perform one of the following to resolve this issue:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Disable name lookup requests from NFS server to a DNS server</strong></p>
<div class="paragraph">
<p>The NFS server attempts to authenticate NFS clients by performing a
reverse DNS lookup to match hostnames in the volume file with the client
IP addresses. There can be a situation where the NFS server either is
not able to connect to the DNS server or the DNS server is taking too
long to responsd to DNS request. These delays can result in delayed
replies from the NFS server to the NFS client resulting in the timeout
error seen above.</p>
</div>
<div class="paragraph">
<p>NFS server provides a work-around that disables DNS requests, instead
relying only on the client IP addresses for authentication. The
following option can be added for successful mounting in such
situations:</p>
</div>
<div class="paragraph">
<p><code>option rpc-auth.addr.namelookup off</code></p>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p><strong>Note</strong>: Remember that disabling the NFS server forces authentication
of clients to use only IP addresses and if the authentication rules in
the volume file use hostnames, those authentication rules will fail and
disallow mounting for those clients.</p>
</div>
</blockquote>
</div>
<div class="paragraph">
<p><strong>OR</strong></p>
</div>
</li>
<li>
<p><strong>NFS version used by the NFS client is other than version 3</strong></p>
<div class="paragraph">
<p>Gluster NFS server supports version 3 of NFS protocol. In recent Linux
kernels, the default NFS version has been changed from 3 to 4. It is
possible that the client machine is unable to connect to the Gluster NFS
server because it is using version 4 messages which are not understood
by Gluster NFS server. The timeout can be resolved by forcing the NFS
client to use version 3. The <strong>vers</strong> option to mount command is used for
this purpose:</p>
</div>
<div class="paragraph">
<p><code>$ mount  -o vers=3</code></p>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="showmount-fails-with-clnt_create-rpc-unable-to-receive"><a class="anchor" href="#showmount-fails-with-clnt_create-rpc-unable-to-receive"></a>showmount fails with clnt_create: RPC: Unable to receive</h4>
<div class="paragraph">
<p>Check your firewall setting to open ports 111 for portmap
requests/replies and Gluster NFS server requests/replies. Gluster NFS
server operates over the following port numbers: 38465, 38466, and
38467.</p>
</div>
<div id="application-fails-with-invalid-argument-or-value-too-large-for-defined-data-type-error." class="paragraph">
<p>Application fails with "Invalid argument" or "Value too large for
defined data type" error.
<sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup>^</p>
</div>
<div class="paragraph">
<p>These two errors generally happen for 32-bit nfs clients or applications
that do not support 64-bit inode numbers or large files. Use the
following option from the CLI to make Gluster NFS return 32-bit inode
numbers instead: nfs.enable-ino32 &lt;on|off&gt;</p>
</div>
<div class="paragraph">
<p>Applications that will benefit are those that were either:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>built 32-bit and run on 32-bit machines such that they do not support
large files by default</p>
</li>
<li>
<p>built 32-bit on 64-bit systems</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This option is disabled by default so NFS returns 64-bit inode numbers
by default.</p>
</div>
<div class="paragraph">
<p>Applications which can be rebuilt from source are recommended to rebuild
using the following flag with gcc:</p>
</div>
<div class="paragraph">
<p><code>-D_FILE_OFFSET_BITS=64</code></p>
</div>
<div class="literalblock">
<div class="content">
<pre>##Troubleshooting File Locks</pre>
</div>
</div>
<div class="paragraph">
<p>In GlusterFS 3.3 you can use <code>statedump</code> command to list the locks held
on files. The statedump output also provides information on each lock
with its range, basename, PID of the application holding the lock, and
so on. You can analyze the output to know about the locks whose
owner/application is no longer running or interested in that lock. After
ensuring that the no application is using the file, you can clear the
lock using the following <code>clear lock</code> commands.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Perform statedump on the volume to view the files that are locked
using the following command:</strong></p>
<div class="paragraph">
<p><code># gluster volume statedump  inode</code></p>
</div>
<div class="paragraph">
<p>For example, to display statedump of test-volume:</p>
</div>
<div class="literalblock">
<div class="content">
<pre># gluster volume statedump test-volume
Volume statedump successful</pre>
</div>
</div>
<div class="paragraph">
<p>The statedump files are created on the brick servers in the`/tmp`
directory or in the directory set using <code>server.statedump-path</code> volume
option. The naming convention of the dump file is
<code>&lt;brick-path&gt;.&lt;brick-pid&gt;.dump</code>.</p>
</div>
<div class="paragraph">
<p>The following are the sample contents of the statedump file. It
indicates that GlusterFS has entered into a state where there is an
entry lock (entrylk) and an inode lock (inodelk). Ensure that those are
stale locks and no resources own them.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>[xlator.features.locks.vol-locks.inode]
path=/
mandatory=0
entrylk-count=1
lock-dump.domain.domain=vol-replicate-0
xlator.feature.locks.lock-dump.domain.entrylk.entrylk[0](ACTIVE)=type=ENTRYLK_WRLCK on basename=file1, pid = 714782904, owner=ffffff2a3c7f0000, transport=0x20e0670, , granted at Mon Feb 27 16:01:01 2012

conn.2.bound_xl./gfs/brick1.hashsize=14057
conn.2.bound_xl./gfs/brick1.name=/gfs/brick1/inode
conn.2.bound_xl./gfs/brick1.lru_limit=16384
conn.2.bound_xl./gfs/brick1.active_size=2
conn.2.bound_xl./gfs/brick1.lru_size=0
conn.2.bound_xl./gfs/brick1.purge_size=0

[conn.2.bound_xl./gfs/brick1.active.1]
gfid=538a3d4a-01b0-4d03-9dc9-843cd8704d07
nlookup=1
ref=2
ia_type=1
[xlator.features.locks.vol-locks.inode]
path=/file1
mandatory=0
inodelk-count=1
lock-dump.domain.domain=vol-replicate-0
inodelk.inodelk[0](ACTIVE)=type=WRITE, whence=0, start=0, len=0, pid = 714787072, owner=00ffff2a3c7f0000, transport=0x20e0670, , granted at Mon Feb 27 16:01:01 2012</pre>
</div>
</div>
</li>
<li>
<p><strong>Clear the lock using the following command:</strong></p>
<div class="paragraph">
<p><code># gluster volume clear-locks</code></p>
</div>
<div class="paragraph">
<p>For example, to clear the entry lock on <code>file1</code> of test-volume:</p>
</div>
<div class="literalblock">
<div class="content">
<pre># gluster volume clear-locks test-volume / kind granted entry file1
Volume clear-locks successful
vol-locks: entry blocked locks=0 granted locks=1</pre>
</div>
</div>
</li>
<li>
<p><strong>Clear the inode lock using the following command:</strong></p>
<div class="paragraph">
<p><code># gluster volume clear-locks</code></p>
</div>
<div class="paragraph">
<p>For example, to clear the inode lock on <code>file1</code> of test-volume:</p>
</div>
<div class="literalblock">
<div class="content">
<pre># gluster  volume clear-locks test-volume /file1 kind granted inode 0,0-0
Volume clear-locks successful
vol-locks: inode blocked locks=0 granted locks=1</pre>
</div>
</div>
<div class="paragraph">
<p>You can perform statedump on test-volume again to verify that the above
inode and entry locks are cleared.</p>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
</div>
      </div>
    </div>
  </div>
   <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
   <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
   <!-- Latest compiled and minified JavaScript -->
   <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
   <script type="text/javascript">
    /*<![CDATA[*/
    $(document).ready(function() {
      $("[id^='topicGroup']").on('show.bs.collapse', function (event) {
        if (!($(event.target).attr('id').match(/^topicSubGroup/))) {
          $(this).parent().find("[id^='tgSpan']").toggleClass("fa-angle-right fa-angle-down");
        }
      });
      $("[id^='topicGroup']").on('hide.bs.collapse', function (event) {
        if (!($(event.target).attr('id').match(/^topicSubGroup/))) {
          $(this).parent().find("[id^='tgSpan']").toggleClass("fa-angle-right fa-angle-down");
        }
      });
      $("[id^='topicSubGroup']").on('show.bs.collapse', function () {
        $(this).parent().find("[id^='sgSpan']").toggleClass("fa-caret-right fa-caret-down");
      });
      $("[id^='topicSubGroup']").on('hide.bs.collapse', function () {
        $(this).parent().find("[id^='sgSpan']").toggleClass("fa-caret-right fa-caret-down");
      });
    });
    /*]]>*/
  </script>
</body>
</html>